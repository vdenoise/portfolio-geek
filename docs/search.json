[
  {
    "objectID": "posts/2022-02-17-gathering_etf_and_stock_historical_data.html",
    "href": "posts/2022-02-17-gathering_etf_and_stock_historical_data.html",
    "title": "Portfolio Geek",
    "section": "",
    "text": "“A quick introduction on how to gather data using Python, Pandas and Pandas_DataReader in order to easily gather historical times from Yahoo Finance.”\n\n\ntoc: false\nbadges: true\ncomments: false\nauthor: Vincent D.\ncategories: [data-gathering]\ntags: [data, time series, yahoo, pandas]\nimage: images/laptop-data.png\nseries: “Finance and Markets with Python and Jupyter: a step-by-step tutorial”\n\n\n\n\n{% series_list%}\nTo start our data analysis and portfolio construction journey, we will perform the basic - but essential - task of getting access to time series and plot them using pandas.\n\nTip: This article is written as a Jupyter Notebook. It has been published using Fastpages. The Jupyter notebook is available on GitHub and if you want to, you can run it directly using the provided Binder link displayed at the top of the article.\n\n\n\n\nA key benefit of Python is the sheer number of libraries we can leverage to perform a particular task. Choosing the right library might look a bit overwhelming, and one the goals of this blog is actually to provide the reader my honest view on what makes most sense to perform the usual tasks in my daily work.\nThe key module for this article is pandas_datareader, a fantastic data gathering library that you can find here on GitHub.\nAnother one performing a similar job with Yahoo Finance would be Yfinance. I will write an article on this later.\n\n#collapse-hide\nfrom matplotlib import pyplot as plt\nimport pandas as pd\nimport pandas_datareader.data as web_reader\nimport datetime as dt\n\n# Note that this change the decimals places inside Jupyter, but not on the website\npd.options.display.float_format = '{:,.1f}'.format\n\n\n\n\nLet’s get ready, and gather some historical data for, say, 4 Exchange Traded Funds (ie “ETFs”, still commonly referred to as “trackers” in France): - SPY: S&P 500 - GLD: Gold - AGG: US Aggregate (US Bonds) - CAC.PA: CAC 40 (French Equity Index)\nYou might wonder what the first 3 letters actually correspond to? They are the usual identifier for each ETF on the markets, and are often called the tickerof the ETF.\nTo gather data, you must de facto provide one ticker for each security, but if you forgot the ticker, Google is usually your friend!\nWe will need define to find a few more parameters: - The 2 variables start_date and end_date to keep some flexibility, - The variable tickers will store our ticker list, and pass it as a parameter to Yahoo, in order to specify our query.\n\n#collapse-hide\n\nstart_date = '2015-01-01'\nend_date = '2022-05-14'\ntickers = ['SPY', 'GLD', 'AGG', 'CAC.PA']\ndf = web_reader.DataReader(tickers, 'yahoo', start=start_date, end=end_date)\n\n# This ones keep the decimals to one on the website. \n# This is especially useful to print dataframes.\ndf = df.round(decimals=2)\n\n\n\n\n\nBy default, Yahoo Finance provides us with several fields, not all of them will be useful in this introduction, and we will perform some further filtering below.\nTo quickly check the status of what we now have, note the use of the tail() function below.\nPandas tail() function\n\ndf.tail(3)\n\n\n\n\n\n  \n    \n      Attributes\n      Adj Close\n      Close\n      High\n      ...\n      Low\n      Open\n      Volume\n    \n    \n      Symbols\n      SPY\n      GLD\n      AGG\n      CAC.PA\n      SPY\n      GLD\n      AGG\n      CAC.PA\n      SPY\n      GLD\n      ...\n      AGG\n      CAC.PA\n      SPY\n      GLD\n      AGG\n      CAC.PA\n      SPY\n      GLD\n      AGG\n      CAC.PA\n    \n    \n      Date\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2022-05-11\n      392.8\n      172.8\n      102.8\n      62.6\n      392.8\n      172.8\n      102.8\n      62.6\n      404.0\n      173.3\n      ...\n      102.0\n      61.3\n      398.1\n      172.5\n      102.1\n      61.7\n      142,361,000.0\n      9,179,600.0\n      16,462,000.0\n      45,387.0\n    \n    \n      2022-05-12\n      392.3\n      170.2\n      103.0\n      62.0\n      392.3\n      170.2\n      103.0\n      62.0\n      395.8\n      172.4\n      ...\n      102.9\n      60.9\n      389.4\n      172.1\n      102.9\n      61.4\n      125,090,800.0\n      11,626,800.0\n      9,015,300.0\n      62,497.0\n    \n    \n      2022-05-13\n      401.7\n      168.8\n      102.5\n      63.5\n      401.7\n      168.8\n      102.5\n      63.5\n      403.2\n      169.6\n      ...\n      102.5\n      62.3\n      396.7\n      168.3\n      102.8\n      62.4\n      104,029,300.0\n      13,026,000.0\n      6,715,600.0\n      77,603.0\n    \n  \n\n3 rows × 24 columns\n\n\n\nThe Adjusted Close field is returned by Yahoo Finance and is exactly what we are looking for.\nAdjusted Close corresponds to the time series containing what we usually call the total return, typically compounding the dividends with the price returns.\nThis reflects the total return delivered by the ETF, should the investor reinvest systematically the dividend paid by the ETF by buying more this ETF. This is probably the most useful field when we aim to assess the long term returns of an asset class.\nTo quickly check the status of what we now have, note the use of the tail() function below, which get the last n rows of the dataset. Combined with Jupyter’s power in printing data, it’s probably the fastest way to navigate and check a particular dataset.\nPandas tail() function\n\ndf['Adj Close'].tail(3)\n\n\n\n\n\n  \n    \n      Symbols\n      SPY\n      GLD\n      AGG\n      CAC.PA\n    \n    \n      Date\n      \n      \n      \n      \n    \n  \n  \n    \n      2022-05-11\n      392.8\n      172.8\n      102.8\n      62.6\n    \n    \n      2022-05-12\n      392.3\n      170.2\n      103.0\n      62.0\n    \n    \n      2022-05-13\n      401.7\n      168.8\n      102.5\n      63.5\n    \n  \n\n\n\n\n\n\n\nTo quickly check that we got the right data, let’s visualise it.\n\n#collapse-hide\nimport seaborn as sns\n\n# Apply the default theme\nsns.set_style('whitegrid')\n\nplt.figure(figsize=(15,6))\np = sns.lineplot(data=df['Adj Close'])\np.set_ylabel(\"Close Price\")\npass\n\n\n\n\n\n\n\nThe chart above is useful, but the vast difference between the ETFs’ values makes it a bit hard to actually track each respective time series.\nIt would be more effective to normalise the data. It’s often referred to as “rebasing”, ie making each time series starting at 100, this will make it much easier to compare.\n\n#collapse-hide\n\n# Normalise the data, which here means for each column to start at 100, with subsequent price development \"scaled\" according to daily returns\nnormalised_ts = (df['Adj Close']/df['Adj Close'].iloc[0, ]*100)\nplt.figure(figsize=(15,6))\np = sns.lineplot(data=normalised_ts)\np.set_ylabel(f\"Close Price, Basis 100 in {start_date}\")\npass\n\n\n\n\n\n\n\nIt’s often easy to get ‘seduced’ by the compelling long term returns, especially about Equities. And indeed, it was a good thing to be invested in Equities in the long run !\nBut 1y returns are usually a good way to keep track of the portfolio, and moreover to see how these returns have developed over time. With a 1y return chart, the notion of risk, ie either fast-changing returns, or - even worse - consistently negative returns, quickly becomes apparent.\n\n# We use pct_change() to calculate the one year return\nts_1y_returns = df['Adj Close'].pct_change(periods=252)\nplt.figure(figsize=(15,6))\np = sns.lineplot(data=ts_1y_returns)\np.set_ylabel(f\"Close Price, Basis 100 in {start_date}\")\npass\n\n\n\n\n\n\n\nThe chart above is great, but a bit too busy for my taste. Whilst accurate, there is too much info going on, the noise might reduce our ability to spot real medium terms or breakouts.\nThis is where pandas starts to be really powerful. The rolling() function essentially captures sub-series, with a defined length (here 21 days). By chaining the results of rolling() with the mean function, ie calculating the arithmetic average, this will provide us in a one-liner with a new series. This generated dataframe contains the time series of the moving average (21 days) for each of our time series.\nWhilst this “chaining” approach might initially sound obscure, this is extremely powerful, especially when factoring in the fact that you just need to change the tickers of the ETFs in the beginning of the article to entirely update the whole analysis … your turn!\n\n# Normalise the data, which here means for each column to start at 100, with subsequent price development \"scaled\" according to daily returns\nts_1y_returns = df['Adj Close'].pct_change(periods=252).rolling(window=21).mean()\nplt.figure(figsize=(15,6))\np = sns.lineplot(data=ts_1y_returns)\np.set_ylabel(f\"Close Price, Basis 100 in {start_date}\")\npass\n\n\n\n\n\n\n\nSo that’s it for this short intro on data gathering and visualising. In this article, we have gathered, checked, normalised and plot close prices for different ETFs.\nThe next step will be to use these function to generate some returns and risk statistics, and start exploring portfolio construction.\nSee you in the next article, and stay safe.\n\n\n\n{% series_list %}\nThere is no such thing as “full tutorial on something”, knowledge is everywhere. I found these tutorials pretty handy, check them out too!\n\nSeaborn Tutorial\nTutorial on F-Strings"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/2022-03-07-optimise-portfolios-with-python.html",
    "href": "posts/2022-03-07-optimise-portfolios-with-python.html",
    "title": "Portfolio Geek",
    "section": "",
    "text": "“In this article, we provide a bird’s eye perspective on how to use Python and a dedicated library called PyPortfolioOpt to put Modern Portfolio Optimisation techniques into practice. In particular, we look at the approach developed in 1952 by Harry Markowitz.”\n\n\ntoc: false\nbadges: true\ncomments: false\nauthor: Vincent D.\ncategories: [portfolio-optimisation]\ntags: [tutorial, markowitz, optimisation, portfolio, construction]\nimage: images/portfolio-optimisation.png\nseries: “Finance and Markets with Python and Jupyter: a step-by-step tutorial”\n\n\n\nTip: This article is written as a Jupyter Notebook. It has been published using Fastpages. The Jupyter notebook is available on GitHub and if you want to, you can run it directly using the provided Binder link displayed at the top of the article.\n\n{% series_list%}\n\n\nHarry Markowitz is one of the (if not “the”) fathers of modern portfolio construction and his seminal paper Portfolio Selection{% fn 1 %} has driven an entire research stream.\nThe intuition behind this paper is that one can combine the information gathered on expected returns, risks and diversification of various assets or asset classes with a view to optimise the risk-return profile of a given portfolio.\nIn practice, we typically find a limited stability of the portfolios generated using this approach, especially due the high sensitivity of the portfolio to the expected returns; nonetheless, the insights it provides are very useful and it’s a perfect start to see the impact of risk and diversification.\nFor this article, we will mostly rely on a fantastic Python library, PyPortfolioOpt {% fn 2%} which will do the optimisation heavy lifting for us.\n\n\n\n\n\nThe main library to load here is indeed PyPortfolioOpt{% fn 2%}, and we will rely extensively on it in this article.\nWe need the following tools as well: - Pandas{% fn 3%}: load, filter, sort and pretty much all data wrangling operations; - Numpy{% fn 4%}: provides most matrix and advanced numerical operations; this library is the calculation backbone for pandas; - Matplotlib{% fn 5%}: the de facto reference library to draw scientific charts; - Pandas_datareader{% fn 6%}: a very handy library to access many different online databases, including Yahoo Finance.\n\n#collapse-hide\nimport numpy as np \nimport pandas_datareader.data as web_reader\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nfrom pypfopt.efficient_frontier import EfficientFrontier\nfrom pypfopt import risk_models\nfrom pypfopt import expected_returns\n\n\n\n\nIn the code below, we set some variables to adjust the jupyter loo\n\n#collapse-hide\n# Note that this change the decimals places inside Jupyter, but not on the website\npd.options.display.float_format = '{:,.1f}'.format\n\n\n\n\n\nFirst and foremost, we need to define our Investable Universe i.e. the set of asset classes that we will allow in our portfolio.\nMany investors would typically think about single stocks, but on my side, I am typically looking I am looking here a Long Term Investing, and I would be keen to\nSpeaking about asset classes, we need some ETFs to analyse!\n\n“Important: Please bear in mind this article if purely for pedagogical purpose and should by no mean be understood as a recommendation or advice. Investing brings risk and in particular risk of loss of capital. I have no intention to recommend anything! I will follow among others this recent article in US News{% fn 7%} and ETF.com{% fn 8%} as sources of inspiration to identify relevant ETFs.”\n\n\n\nLet’s start with various Equity sub-asset classes:\n\nIVV: iShares Core S&P 500 ETF representing US Equity Large Cap\nSCHA: Schwab U.S. Small-Cap ETF representing US Equity Small Cap\nIJH: iShares Core S&P Mid-Cap ETF representing US Equity Mid Cap\nSCHD: Schwab U.S. Dividend Equity ETF representing US Equity Dividend (ie US stocks which are deemed to pay higher dividends)\nVTI: Vanguard Total Stock Market ETF representing US Equity, with all market cap included\nVXUS: Vanguard Total International Stock ETF representing World ex-US equities\nEEM: iShares MSCI Emerging Markets ETF representing the Emerging Market Equities\n\n\n\n\nLet’s add Fixed Income, which would typically aim to reduce the overall portfolio’s volatility:\n\nAGG: iShares Core U.S. Aggregate Bond ETFrepresenting the entire US Bond market\nGOVT: iShares U.S. Treasury Bond ETF representing the performance of US Government Bonds\nVCLT: Vanguard Long-Term Corporate Bond ETF representing the Investment Grade USD denominated bonds.\n\n\n\n\nLet’s add 2 additional asset classes, Gold and Commodities: - GLD: SPDR Gold Trust representing the price of Gold - PDBC: Invesco Optimum Yield Diversified Commodity** Strategy No K-1 ETF** which will represent the performance of the Broad Commodities asset class\nEverything in the above is denominated in US Dollars, this will make our life easier in what follows, ie we will not need any currency conversion, which is always a bit painfull in the process.\n\n\n\n\nI recommend you to read this article about data gathering.\nLike we did in this article, we will utilise pandas_datareader{% fn 6%} to get historical time series. As mentioned above, we will look at the performance of ETFs, which we will consider as as relevant proxies for their respective asset classes.\nYou can of course utilise whatever asset class you want, and take single stocks, funds as historical data points. On my side, I am quite familiar with indices ETFs, and I will go with the selection above.\nLast but not least, we need to define the start_date and end_date for gathering the historical time series. For this study, we will gather 7 years of data.\n\n#collapse-hide\n\nstart_date = '2015-03-01'\nend_date = '2022-03-01'\n\n# Define Investable Universe\ninvestable_universe_tickers = ['IVV', 'SCHA', 'IJH', 'SCHD', 'VTI', 'VXUS', 'EEM', 'VCLT', 'AGG', 'GOVT','PDBC','GLD']\n\n# Get Historical Data\ndf = web_reader.DataReader(investable_universe_tickers, 'yahoo', start=start_date, end=end_date)\ndf = df['Adj Close']\n\n# This is required to round the blog's table into 2 decimals, Jupyter's formatting does not apply on the published website\ndf = df.round(decimals=1)\n\n\n\nThe request above delivered a pandas data_frame, and here is a snapshot of the last 5 rows:\n\n#collapse-hide\ndf.tail(3)\n\n\n\n\n\n  \n    \n      Symbols\n      IVV\n      SCHA\n      IJH\n      SCHD\n      VTI\n      VXUS\n      EEM\n      VCLT\n      AGG\n      GOVT\n      PDBC\n      GLD\n    \n    \n      Date\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2022-02-25\n      438.2\n      46.8\n      264.9\n      76.9\n      220.7\n      60.7\n      47.3\n      94.8\n      109.1\n      25.5\n      15.9\n      176.6\n    \n    \n      2022-02-28\n      437.3\n      47.0\n      264.9\n      76.6\n      220.5\n      59.9\n      46.7\n      96.3\n      109.8\n      25.7\n      16.2\n      178.4\n    \n    \n      2022-03-01\n      430.5\n      46.1\n      259.7\n      75.2\n      217.0\n      58.8\n      46.1\n      96.3\n      110.5\n      25.9\n      16.9\n      181.6\n    \n  \n\n\n\n\n\n\n\nA table like the above is not very useful: in the absence a of particle knowledge of the ETFs’ values, we have no way of knowing if a value is “high” or “low”, hence apart from telling us that the value is a number we have now way to let’s try to make this table a bit more insightful.\nThe request above delivered a data_frame, and here is a snapshot of the last 5 rows:\n\n#collapse-hide\n# Same table, but this time, normalised\n(df/df.iloc[0, ]*100).round(decimals=1).tail(3)\n\n\n\n\n\n  \n    \n      Symbols\n      IVV\n      SCHA\n      IJH\n      SCHD\n      VTI\n      VXUS\n      EEM\n      VCLT\n      AGG\n      GOVT\n      PDBC\n      GLD\n    \n    \n      Date\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2022-02-25\n      234.7\n      180.0\n      194.9\n      235.9\n      229.2\n      144.9\n      134.8\n      135.8\n      117.1\n      112.8\n      133.6\n      152.6\n    \n    \n      2022-02-28\n      234.2\n      180.8\n      194.9\n      235.0\n      229.0\n      143.0\n      133.0\n      138.0\n      117.8\n      113.7\n      136.1\n      154.2\n    \n    \n      2022-03-01\n      230.6\n      177.3\n      191.1\n      230.7\n      225.3\n      140.3\n      131.3\n      138.0\n      118.6\n      114.6\n      142.0\n      157.0\n    \n  \n\n\n\n\nThis is much better.\nNow we can at least see that: - US Equities (e.g. IVV) had a fantastic ride since 2015; - Government bonds (e.g. GOVT) under-performed; - Commodities (PDBC) and Gold (GLD) had several rough years as well, with recent massive pick-up in the current geopolitical context.\n\n\n\nBeing able to sanity check the data is very important, and it’s often more efficient with a quick chart.\n\n#collapse-hide\n\n# Normalise to 100\nnormalised_data = (df/df.iloc[0, ]*100)\n\n# A bit of data wrangling\ntransposed_data = normalised_data.tail(1).reset_index().transpose()\ntransposed_data = transposed_data.iloc[1:len(transposed_data)]\ntransposed_data = transposed_data.rename(columns={0: \"Last Value\"})\n\n# We want a bar chart sorted by decreaseing values\ntransposed_data = transposed_data.sort_values(by=\"Last Value\", ascending=False)\n\n# Theming Seaborn results\nsns.set_theme()\n\n# Draw\ntransposed_data.plot.bar(figsize=(10, 6))\npass\n\n\n\n\n\n\n\nThe table above is useful, but when it comes to grasping and long term risks and returns, a chart is worth a thousand words.\nA very common issue when charting multiple time series is the very different stock levels, and this can make the chart hard to read.\nThis is why we will once again normalise the data.\n\n#collapse-hide\n\n# Theming Seaborn results\nsns.set_theme()\n\n# Plot the time series\nplt.figure(figsize=(12,6))\n\n# Legends and Axis titles\np = sns.lineplot(data=normalised_data)\np.set_ylabel(f\"Close Price, Basis 100 in {start_date}\")\npass\n\n\n\n\n\n\n\n\nIn Markowitz 1952{% fn 1 %}, the optimal portfolio is obtained as a function of expected returns and expected risks of the portfolio. This takes a strong assumption that we have a “crystal ball”, or at least access to a predictive model, which at this stage is well beyond this article.\nFor this first example, we will rely on historical parameters estimation, which precisely means that we expect the past to be a good prediction of what might happen in the future.\n\n\nThere are many ways to estimate the volatility, and I will only cover the simplest approach.\nWe have access to daily close prices of the ETFs, hence we could look at the standard deviation of daily returns. In theory this would utilise most of the data we have access to, which is a good thing. In practice, and especially when looking at asset classes which can be observed with an 8 to 12 hours time difference which might lead the “sample volatility” to become inconsistent between 2 asset classes. This is even more important for estimating the correlation.\nPractitioners often use weekly returns to alleviate this issue, and we will do the same here, and as such calculate the returns using a resampled time series. To calculate the annualized volatility requires an additional factor in this case the square root of 52. Why this?\nVolatility is essential to option traders, and when pricing options, practitioners often model asset prices as Wiener processes(number of weeks in a year) {% fn 9 %}. The variance of a Wiener process is proportional to the time, and the volatility is the square root of the the variance, which gives us that to convert standard deviation of weekly returns into an annualized figure, we need to multiply our results by the square root of the number of weeks in a year (more on Wiener processes here {% fn 9 %}).\n\nweekly_returns = df.resample(\"W\").last().pct_change()\n((weekly_returns.std()*math.sqrt(52)*100).sort_values()).plot.bar();\n\n\n\n\nAs one could have guessed, on the left of the chart above, we can find the low volatility asset classes (e.g. Government Bonds, Investment Grade Bonds), whereas on the right we have high volatility asset classes (Mid Cap and Small Cap Equities).\n\n\n\n\n\nsns.heatmap(weekly_returns.corr());\n\n\n\n\n\n#collapse-hide\nfrom pypfopt import risk_models\nfrom pypfopt import plotting\n\n# Calculate expected returns and sample covariance\nmu = expected_returns.mean_historical_return(df)\nsample_cov = risk_models.sample_cov(df, frequency=252)\n\nS = risk_models.CovarianceShrinkage(df).ledoit_wolf()\n\n\n\n\n\n\nWith expected returns and risk estimated, we are ready to utilise PyPortfolioOpt’s optimiser to draw the efficient frontier.\nThe idea behind the efficient frontier{% fn 1 %} is relatively simple: - For each level of investor’s risk, there is an optimal portfolio which is expected to deliver the highest level of return; or conversely - For each level of investor’s return, there is an optimal portfolio which is expected to deliver the lowest level of return.\n\n#collapse-hide\nef = EfficientFrontier(mu, sample_cov)\n\n# We create 2 efficient frontiers\nfig, ax = plt.subplots()\nplotting.plot_efficient_frontier(ef, ax=ax, show_assets=True)\nax.set_title(\"Asset Classes & Efficient Frontier\")\n\nplt.show()\n\n\n\n\n\n\n\n\n#collapse-hide\n\n# Find and plot the tangency portfolio\nfig2, ax = plt.subplots()\nef2 = EfficientFrontier(mu, S) \nplotting.plot_efficient_frontier(ef2, ax=ax, show_assets=True)\n\nef3 = EfficientFrontier(mu, S) \n\nef3.max_sharpe()\nret_tangent, std_tangent, _ = ef3.portfolio_performance()\nax.scatter(std_tangent, ret_tangent, marker=\"*\", s=100, c=\"r\", label=\"Max Sharpe\")\nax.set_title(\"Asset Classes, Efficient Frontier & Max Sharpe portfolio\")\nplt.show()\n\n\n\n\n\n\n\n\n#collapse-hide\n\n# Plot random portfolios\nfig2, ax = plt.subplots()\nn_samples = 10000\nw = np.random.dirichlet(np.ones(len(mu)), n_samples)\nrets = w.dot(mu)\nstds = np.sqrt((w.T * (S @ w.T)).sum(axis=0))\nsharpes = rets / stds\nax.scatter(stds, rets, marker=\".\", c=sharpes, cmap=\"viridis_r\")\n\n# Format\nax.set_title(\"Random portfolios, based on the same asset classes\")\n#ax.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nEt voila!\nCompared to macros or Excel formulas, this is an amazing improvement, and enables us to customise everything in seconds: historical timeframe, asset classes, etc.\nTo deliver a full portfolio optimisation process in such a limited number of lines is truly awesome.\nI hope this article provided you some ideas on how to look at portfolio construction.\nHappy coding, and happy portfolio construction!\n{% series_list %}\n\n\n\n{{ ‘Portfolio Selection, Henri Markowitz, 1952’ | fndetail: 1 }}\n{{ ‘PyportfolioOpt, Robert Andrew Martin, 2018’ | fndetail: 2}}\n{{ ‘Pandas’ | fndetail: 3 }}\n{{ ‘Numpy’ | fndetail: 4 }}\n{{ ‘Matplolib’ | fndetail: 5}}\n{{ ‘Pandas_datareader’ | fndetail: 6}}\n{{ ‘US News, 7 Best Long-Term ETFs to Buy and Hold, 25 Feb 2022’ | fndetail: 7}}\n{{ ‘ETF.com’ | fndetail: 8}}\n{{ ‘Wikipedia.com / Wiener Processes’ | fndetail: 9}}"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/2022-09-19-estimating-portfolio-risk.html",
    "href": "posts/2022-09-19-estimating-portfolio-risk.html",
    "title": "Portfolio Geek",
    "section": "",
    "text": "“A quick introduction on how to measure and model financial risk, with the angle of historical volatility.”\n\n\ntoc: false\nbadges: true\ncomments: false\nauthor: Vincent D.\ncategories: [data-gathering]\ntags: [data, time series, yahoo, pandas]\nimage: images/laptop-data.png\nseries: “Portfolio Construction 101”\n\n\n\nIn today’s session, we will\n\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport pandas_datareader as web_reader\nimport matplotx\nimport matplotlib.dates as mdates\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import norm\nimport math\nimport seaborn as sns\n\ntickers = ['SPY']\n\nstart_date = '2010-09-14'\nend_date = '2022-09-14'\ntickers = ['SPY', 'AGG']\n\ndf = web_reader.DataReader(tickers, 'yahoo', start=start_date, end=end_date)\ndata = df.Close.SPY.resample(\"B\").last().fillna(method=\"bfill\")\ndata_agg = df.Close.AGG.resample(\"B\").last().fillna(method=\"bfill\")\n\n\n\n\n\nplt.style.use(\"./stylesheets/dracula_slide.mplstyle\")\ndata.resample(\"W\").last().plot(figsize=(4,6));\nplt.ylabel(\"Wealth Curve\")  \nmatplotx.line_labels() \nmatplotx.xlabels() \n\nplt.savefig(\"output/wealth_curve_spy.png\" );\n\nAttributeError: module 'matplotx' has no attribute 'xlabels'\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(8, 6))\ndata_bar = data.resample(\"W\").last().pct_change().multiply(100)\nindex = data_bar.index\nindex = [pd.to_datetime(date, format='%Y-%m-%d').date() for date in index]\ndata_bar.index = index\nax = data_bar.plot.bar()\nplt.ylabel(\"Weekly Return (%)\")\nax.xaxis.set_major_locator(mdates.MonthLocator(interval=3))\nplt.gcf().autofmt_xdate()\nplt.savefig(\"output/weekly_returns_spy.png\" );\n\n\n\n\n\n\n\n\nsorted_returns.plot(figsize=(8, 6));\nplt.savefig(\"output/sorted_weekly_returns_spy.png\" );\n\n\n\n\n\n\n\n\ndata_bar.hist(bins=100, figsize=(8,6));\n\n\n\n\n\nplt.figure(figsize=(8, 6))\nax = sns.kdeplot(data_bar)\nmean = data_bar.mean()\nstd = data_bar.std()\nN = 10\nfor i in [1, 2, 3]:\n    x1 = np.linspace(mean - i*std, mean - (i - 1)*std, N)\n    x2 = np.linspace(mean - (i - 1)*std, mean + (i - 1)*std, N)\n    x3 = np.linspace(mean + (i - 1)*std, mean + i*std, N)\n    x = np.concatenate((x1, x2, x3))\n    x = np.where((mean - (i - 1)*std < x) & (x < mean + (i - 1)*std), np.nan, x)\n    y = norm.pdf(x, mean, std)\n    ax.fill_between(x, y, alpha=0.5)\n\nplt.xlabel(\"SPY Weekly Returns (in %, US Dollar)\")\nplt.ylabel(\"Probability Density Function\")\n#plt.xticks(ticks=range(0, 10))\nplt.grid()\n\nplt.show()\n\n\n\n\n\n\n\n\nplt.figure(figsize=(8, 6))\nplt.savefig(\"output/wealth_curve_spy.png\" );\ndata.resample(\"W\").last().pct_change().rolling(52).std().multiply(math.sqrt(52)).plot(secondary_y=True)\nmatplotx.line_labels() \nplt.savefig(\"output/wealth_curve_spy_with_vol.png\" );\n\n\n\n\n\nplt.figure(figsize=(8, 6))\ndata.resample(\"W\").last().pct_change().rolling(52).std().multiply(math.sqrt(52)).plot(secondary_y=False)\nmatplotx.line_labels()\nplt.savefig(\"output/vol_spy.png\" );\ndata_agg.resample(\"W\").last().pct_change().rolling(52).std().multiply(math.sqrt(52)).plot(secondary_y=False)\nmatplotx.line_labels()\nplt.savefig(\"output/vol_spy_with_agg.png\" );"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Portfolio Geek",
    "section": "",
    "text": "Visualizing historical data\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nPortfolio optimisation with Python\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimate Financial Risk\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nPost With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nDec 5, 2022\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nDec 2, 2022\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "With 20 years of experience in portfolio construction, asset allocation and quantitative research, I have pretty much worked across the entire value chain of the asset management and capital markets industry.\nIn my different roles, I have developed and implemented a wide range range of investment solutions, ranging from Exchange Traded Funds (ETFs), derivatives and investment mandates across most asset classes: Equities, Fixed Income, Commodities, Alternative Assets, Multi-Asset portfolios.\nFind me on Linkedin\n\nWhy this blog?\nWhat struck me the most when I started my career in finance was the knowledge asymetry between finance professional and end clients. Just to clarify, I don’t mean IQ asymmetry, only a difference in knowledge (e.g. data, jargon, formulas, etc.). Whilst this gap reduced considerably since then, thanks to internet and better information provided, many concepts might still sound fairly exotic or technical to many investors.\nHere, I will try to demystify financial analysis and provide ideas and starting points in order to put theory into practice.\nIn most articles, you should be able to copy, run and modify the code behind the analysis. This should help you to directly use many of the tools presented.\n\n\nMain Publications\nBelow are the main research papers I have published or participated to in the last few years, feel free to take a look."
  }
]